# Dockerized Scraping & Dashboard System with Database Migrations

## 1. High-Level Architecture (Containerized with Migration Support)

This system is designed to run in Docker containers with robust database migration capabilities using Alembic. Redis handles task queuing, PostgreSQL provides persistent storage, and Alembic manages schema evolution.

```text
┌───────────────────────────────────────────────────────────────────────┐
│                         DOCKER COMPOSE ENVIRONMENT                    │
│                                                                       │
│  1. DATABASE & QUEUE SERVICES                                         │
│  ┌─────────────────┐       ┌───────────────────────────┐              │
│  │   PostgreSQL    │       │         Redis             │              │
│  │  (Data Storage) │       │   (Broker & Cache)        │              │
│  └─────────────────┘       └─────────────┬─────────────┘              │
│                                          │                            │
│  2. APPLICATION SERVICES                 │                            │
│  ┌───────────────────────────────────────▼──────────────────────────┐ │
│  │   Celery Worker Container                                        │ │
│  │   [Scraping | robots.txt | Source Management | DB Logging]       │ │
│  └───────────────────────────────┬──────────────────────────────────┘ │
│                                  │                                    │
│  ┌───────────────────────────────▼──────────────────────────────────┐ │
│  │   Celery Beat Container                                          │ │
│  │   (Task Scheduler - Optional)                                    │ │
│  └──────────────────────────────────────────────────────────────────┘ │
│                                  │                                    │
│  ┌───────────────────────────────▼──────────────────────────────────┐ │
│  │   Streamlit Dashboard Container                                  │ │
│  │   (Real-time Monitoring & Control)                               │ │
│  └──────────────────────────────────────────────────────────────────┘ │
│                                                                       │
│  3. DATABASE MIGRATION SYSTEM                                         │
│  ┌──────────────────────────────────────────────────────────────────┐ │
│  │   Alembic Migrations                                             │ │
│  │   - Version-controlled schema changes                            │ │
│  │   - Automated database upgrades/downgrades                       │ │
│  │   - Environment-aware configuration                              │ │
│  └──────────────────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────────────────┘
```

## 2. Technology Stack

*   **Containerization:** Docker, Docker Compose
*   **Base Image:** Python 3.10-slim
*   **Database:** PostgreSQL 15 (Containerized)
*   **Queue/Broker:** Redis 7 (Containerized)
*   **Task Queue:** Celery with Redis backend
*   **Database Migrations:** Alembic
*   **Scraping:** `requests`, `BeautifulSoup4`, `urllib.robotparser`
*   **Dashboard:** Streamlit
*   **ORM:** SQLAlchemy
*   **Configuration:** Pydantic Settings, python-dotenv

## 3. Enhanced Data Flow with Source Management

1.  **Task Submission:** URLs submitted via Streamlit dashboard UI
2.  **Queueing:** Tasks placed in Redis queue via Celery
3.  **Source Registration:** Automatic domain extraction and source table population
4.  **Compliance Check:** Mandatory `robots.txt` verification before scraping
5.  **Fetch:** HTTP request with proper headers and timeout handling
6.  **Parse:** HTML extraction using BeautifulSoup
7.  **Data Enrichment:** Structured data extraction (headings, links, metadata)
8.  **Persistence:** Upsert into PostgreSQL with source relationship
9.  **Logging:** Structured logging to database for audit and debugging
10. **Visualization:** Streamlit queries database and displays analytics

## 4. Key Components Detail

### 4.1 Enhanced Database Schema

```sql
-- sources table (domain tracking)
CREATE TABLE sources (
    id SERIAL PRIMARY KEY,
    domain VARCHAR(255) UNIQUE NOT NULL,
    robots_url VARCHAR(255),
    last_crawled TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- scraped_data table (main data storage)
CREATE TABLE scraped_data (
    id SERIAL PRIMARY KEY,
    source_id INTEGER REFERENCES sources(id),
    url VARCHAR UNIQUE NOT NULL,
    title VARCHAR,
    content JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- logs table (audit trail)
CREATE TABLE logs (
    id SERIAL PRIMARY KEY,
    level VARCHAR(50),
    task_id VARCHAR,
    url VARCHAR,
    message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 4.2 Container Services Architecture

| Service | Image | Purpose | Ports | Dependencies |
|---------|-------|---------|-------|--------------|
| postgres | postgres:15 | Primary database storage | 5432 (internal) | - |
| redis | redis:7 | Message broker & cache | 6379 (internal) | - |
| worker | Custom build | Scraping engine with source management | - | postgres, redis |
| dashboard | Custom build | Monitoring & control UI | 8501:8501 | postgres |
| beat | Custom build | Scheduled task execution | - | redis |

### 4.3 Migration System (Alembic)

*   **Configuration:** `alembic.ini` with environment-aware settings
*   **Migration Directory:** `migrations/versions/` for version-controlled schema changes
*   **Environment Integration:** Automatic detection of Docker vs localhost environments
*   **Initial Migration:** Creates complete schema with sources, logs, and scraped_data tables
*   **Future Migrations:** Template-based generation for schema evolution

### 4.4 Enhanced Scraper Logic

*   **Source Management:**
    *   Automatic domain extraction from URLs
    *   Source table population with domain tracking
    *   Timestamp updates for last_crawled
*   **Compliance Module:** `src/scraper/compliance.py`
    *   `urllib.robotparser` for robots.txt validation
    *   Fail-open policy for inaccessible robots.txt
*   **Structured Logging:**
    *   Database-backed log storage
    *   Task-level logging with severity levels
    *   Error tracking and audit trail
*   **Data Relationships:**
    *   Foreign key relationship between scraped_data and sources
    *   Automatic source_id assignment

### 4.5 Enhanced Dashboard (Streamlit)

*   **Main Page:**
    *   Real-time data table with scraping results
    *   Analytics: Links-per-page bar chart
    *   JSON inspector for detailed content viewing
*   **Control Panel:**
    *   Bulk URL submission (one per line)
    *   Manual task triggering
    *   Data refresh controls
*   **Future Enhancements:**
    *   Source management interface
    *   Log viewer with filtering
    *   System health monitoring

## 5. Project Structure

```
scrapping-project-for-abtin/
├── Dockerfile                    # Single Dockerfile for all Python services
├── docker-compose.yml            # Multi-service orchestration
├── alembic.ini                   # Alembic configuration
├── migrations/                   # Database migration scripts
│   ├── env.py                    # Migration environment configuration
│   ├── script.py.mako            # Migration template
│   └── versions/                 # Versioned migration files
│       └── 51677a572fd5_initial_schema_with_sources_and_logs.py
├── .env.example                  # Environment template
├── requirements.txt              # Python dependencies (now includes alembic)
├── src/                          # Application source code
│   ├── config.py                 # Configuration management
│   ├── database/                 # Database layer
│   │   ├── models.py            # SQLAlchemy models (enhanced with Source/Log)
│   │   ├── connection.py        # Database session management
│   │   └── __init__.py
│   ├── scraper/                  # Scraping logic
│   │   ├── tasks.py             # Enhanced Celery tasks with source/logging
│   │   ├── compliance.py        # robots.txt checking
│   │   └── __init__.py
│   ├── dashboard/                # Streamlit UI
│   │   └── app.py               # Dashboard application
│   └── __init__.py
└── logs/                         # Application logs directory
```

## 6. Database Migration Workflow

### Initial Setup
```bash
# 1. Start services
docker-compose up -d postgres redis

# 2. Run initial migration
docker-compose run --rm worker alembic upgrade head

# 3. Verify tables created
docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "\dt"
```

### Creating New Migrations
```bash
# 1. Generate new migration after model changes
docker-compose run --rm worker alembic revision --autogenerate -m "description"

# 2. Apply migration
docker-compose run --rm worker alembic upgrade head

# 3. Rollback if needed
docker-compose run --rm worker alembic downgrade -1
```

### Migration Commands Reference
```bash
# Check current revision
docker-compose run --rm worker alembic current

# Show migration history
docker-compose run --rm worker alembic history

# Apply specific revision
docker-compose run --rm worker alembic upgrade <revision_id>

# Create empty migration
docker-compose run --rm worker alembic revision -m "manual_change"
```

## 7. Deployment Commands (Docker-Based)

```bash
# 1. Clone and setup project
git clone <repository-url>
cd scrapping-project-for-abtin

# 2. Create environment file
cp .env.example .env
# Edit .env with your configuration (POSTGRES_HOST, REDIS_URL, etc.)

# 3. Build and start all services
docker-compose up --build -d

# 4. Initialize database with migrations
docker-compose run --rm worker alembic upgrade head

# 5. Monitor services
docker-compose logs -f [service_name]  # worker, dashboard, postgres, redis

# 6. Access services:
# - Dashboard: http://localhost:8501
# - PostgreSQL: localhost:5432
# - Redis: localhost:6379

# 7. Stop services
docker-compose down

# 8. Complete cleanup (removes volumes)
docker-compose down -v

# 9. Scale workers
docker-compose up -d --scale worker=3
```

## 8. Development Workflow

### Local Development
```bash
# 1. Start services with hot-reload
docker-compose up

# 2. Make code changes - they automatically reflect due to volume mounting

# 3. Database changes workflow:
#    a. Update models.py
#    b. Generate migration: docker-compose run --rm worker alembic revision --autogenerate -m "description"
#    c. Review generated migration file
#    d. Apply: docker-compose run --rm worker alembic upgrade head

# 4. Test new features
#    - Add URLs via dashboard
#    - Monitor logs: docker-compose logs -f worker
#    - Check database: docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB
```

### Production Considerations
1.  **Migration Strategy:**
    *   Always test migrations in staging first
    *   Use `--sql` flag to preview SQL before applying
    *   Implement rollback procedures
2.  **Data Integrity:**
    *   Backup database before migrations
    *   Use transactions for data migration steps
3.  **Security:**
    *   Secure .env file with production credentials
    *   Consider Docker secrets for sensitive data
    *   Implement network policies between containers
4.  **Monitoring:**
    *   Monitor migration status and errors
    *   Track database performance metrics
    *   Set up alerts for failed migrations

## 9. Enhanced Features

### Source Management
- **Automatic Registration:** New domains automatically added to sources table
- **Activity Tracking:** Last crawled timestamp updated with each scrape
- **Status Management:** is_active flag for enabling/disabling sources
- **Relationship Mapping:** Foreign key linking scraped data to sources

### Structured Logging
- **Database Storage:** Logs persisted in PostgreSQL for long-term retention
- **Task Correlation:** Log entries linked to Celery task IDs
- **Severity Levels:** INFO, WARN, ERROR for filtering
- **URL Context:** Each log entry associated with specific URL

### Audit Trail
- **Complete History:** All scraping activities logged
- **Error Tracking:** Detailed error messages for debugging
- **Performance Metrics:** Scraping duration and success rates
- **Compliance Monitoring:** robots.txt violations tracked

## 10. Future Migration Planning

### Common Migration Patterns
```python
# 1. Add new column
op.add_column('scraped_data', sa.Column('updated_at', sa.DateTime()))

# 2. Add foreign key constraint
op.create_foreign_key('fk_scraped_data_source', 'scraped_data', 'sources', ['source_id'], ['id'])

# 3. Data migration
def upgrade():
    # Schema changes
    op.add_column('sources', sa.Column('scrape_interval', sa.Integer(), nullable=True))
    
    # Data updates
    connection = op.get_bind()
    connection.execute(
        "UPDATE sources SET scrape_interval = 3600 WHERE scrape_interval IS NULL"
    )
```

### Migration Best Practices
1.  **Keep Migrations Small:** One logical change per migration
2.  **Test Rollbacks:** Ensure downgrade() functions correctly
3.  **Avoid Raw SQL:** Use SQLAlchemy operations when possible
4.  **Document Changes:** Clear migration messages
5.  **Review Generated Code:** Always inspect autogenerated migrations

## 11. Monitoring & Troubleshooting

### Key Health Indicators
1.  **Migration Status:** Current database revision vs expected
2.  **Source Tracking:** Number of active sources and last crawl times
3.  **Queue Depth:** Pending tasks in Redis
4.  **Success Rate:** Successful vs failed scraping tasks
5.  **Log Volume:** INFO vs ERROR log distribution

### Common Issues & Solutions
```bash
# Migration conflicts
docker-compose run --rm worker alembic current
docker-compose run --rm worker alembic heads
docker-compose run --rm worker alembic merge heads

# Database connection issues
docker-compose exec postgres pg_isready -U $POSTGRES_USER -d $POSTGRES_DB

# View recent logs
docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "SELECT * FROM logs ORDER BY created_at DESC LIMIT 10;"

# Check source statistics
docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "SELECT domain, COUNT(*) as scrape_count FROM sources JOIN scraped_data ON sources.id = scraped_data.source_id GROUP BY domain;"
```

---