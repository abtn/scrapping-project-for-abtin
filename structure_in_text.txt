# High-Performance Async Scraping Engine with Gevent Pool

## 1. High-Level Architecture (Gevent-Based Concurrent Processing)

This system has been transformed into a modular, asynchronous, and fault-tolerant scraping engine running on a high-performance gevent worker pool. The architecture leverages coroutine-based concurrency for massive parallelism while maintaining database integrity and comprehensive logging.

```text
┌───────────────────────────────────────────────────────────────────────────────┐
│                         HIGH-PERFORMANCE SCRAPING ENGINE                      │
│                                                                               │
│  1. INFRASTRUCTURE LAYER                                                      │
│  ┌─────────────────┐       ┌───────────────────────────┐                      │
│  │   PostgreSQL    │       │         Redis             │                      │
│  │  (Data Storage) │       │   (High-Speed Broker)     │                      │
│  └─────────────────┘       └─────────────┬─────────────┘                      │
│                                          │                                    │
│  2. TASK PROCESSING LAYER (GREENLET POOL)│                                    │
│  ┌───────────────────────────────────────▼──────────────────────────────────┐ │
│  │   Celery Worker with Gevent Pool (50 concurrent tasks)                   │ │
│  │                                                                          │ │
│  │  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐     │ │
│  │  │ Greenlet 1   │ │ Greenlet 2   │ │ Greenlet 3   │ │ Greenlet 50  │     │ │
│  │  │ - Fetch      │ │ - Parse      │ │ - Compliance │ │ - Save       │     │ │
│  │  │ - Retry      │ │ - Extract    │ │ - Check      │ │ - Log        │     │ │
│  │  └──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘     │ │
│  │                                                                          │ │
│  │  All greenlets share:                                                    │ │
│  │  • Database connections                                                  │ │
│  │  • HTTP sessions                                                         │ │
│  │  • Parser instances                                                      │ │
│  │                                                                          │ │
│  └───────────────────────────────────────┬──────────────────────────────────┘ │
│                                          │                                    │
│  3. APPLICATION LAYER (MODULAR DESIGN)   │                                    │
│  ┌───────────────────────────────────────▼──────────────────────────────────┐ │
│  │   Modular Components                                                     │ │
│  │  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐             │ │
│  │  │   Fetcher       │ │    Parser       │ │   Compliance    │             │ │
│  │  │ • Retry logic   │ │ • Generic       │ │ • robots.txt    │             │ │
│  │  │ • UA rotation   │ │ • Extensible    │ │ • Domain check  │             │ │
│  │  │ • Session mgmt  │ │ • Schema-based  │ │ • Cache         │             │ │
│  │  └─────────────────┘ └─────────────────┘ └─────────────────┘             │ │
│  │                                                                          │ │
│  │  ┌─────────────────┐ ┌─────────────────┐                                 │ │
│  │  │   Database      │ │    Logger       │                                 │ │
│  │  │ • Source mgmt   │ │ • Structured    │                                 │ │
│  │  │ • Upsert logic  │ │ • Task-linked   │                                 │ │
│  │  │ • Relations     │ │ • Audit trail   │                                 │ │
│  │  └─────────────────┘ └─────────────────┘                                 │ │
│  └──────────────────────────────────────────────────────────────────────────┘ │
│                                                                               │
│  4. CONTROL & MONITORING LAYER                                                │
│  ┌──────────────────────────────────────────────────────────────────────────┐ │
│  │   Streamlit Dashboard                                                    │ │
│  │   • Real-time queue monitoring                                           │ │
│  │   • Performance metrics                                                  │ │
│  │   • Batch URL submission                                                 │ │
│  │   • Error analysis                                                       │ │
│  └──────────────────────────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────────────────────────┘
```

## 2. Technology Stack (Enhanced for High Performance)

*   **Containerization:** Docker, Docker Compose
*   **Base Image:** Python 3.10-slim
*   **Concurrency Model:** Gevent (greenlet-based async)
*   **Database:** PostgreSQL 15 (Containerized)
*   **Queue/Broker:** Redis 7 (Containerized)
*   **Task Queue:** Celery with Redis backend + Gevent pool
*   **Database Migrations:** Alembic
*   **HTTP Client:** Requests with session pooling
*   **HTML Parsing:** BeautifulSoup4 with modular parsers
*   **Retry Logic:** Tenacity with exponential backoff
*   **Dashboard:** Streamlit with real-time updates
*   **ORM:** SQLAlchemy with connection pooling
*   **Configuration:** Pydantic Settings, python-dotenv

## 3. Performance Architecture

### 3.1 Gevent Worker Pool Configuration
```
Worker Configuration:
• Pool Type: Gevent (coroutine-based)
• Concurrency: 50 greenlets
• Throughput: ~50 concurrent HTTP requests
• Memory: ~2-3MB per greenlet (lightweight)
• I/O: Non-blocking network operations
• Scaling: Horizontal scaling possible via --scale worker=3
```

### 3.2 Modular Component Design
```
┌─────────────────────────────────────────────────────────┐
│                    TASK EXECUTION FLOW                  │
├─────────────────────────────────────────────────────────┤
│ 1. Task Queue (Redis)                                   │
│    ↓ 50 concurrent tasks                                │
│ 2. Gevent Worker Pool                                   │
│    ↓ Each greenlet runs:                                │
│ 3. Fetcher Module                                       │
│    • User-Agent rotation                                │
│    • Retry with exponential backoff                     │
│    • Connection pooling                                 │
│ 4. Compliance Check                                     │
│    • robots.txt validation                              │
│    • Domain registry                                    │
│ 5. Parser Module                                        │
│    • Generic HTML extraction                            │
│    • Schema-based data extraction                       │
│ 6. Database Module                                      │
│    • Source management                                  │
│    • Upsert operations                                  │
│    • Relationship mapping                               │
│ 7. Logger Module                                        │
│    • Structured logging                                 │
│    • Audit trail                                        │
└─────────────────────────────────────────────────────────┘
```

## 4. Enhanced Project Structure

```
scrapping-project-for-abtin/
├── Dockerfile                    # Python 3.10-slim with all dependencies
├── docker-compose.yml            # Multi-service orchestration (Gevent pool: --concurrency=50)
├── alembic.ini                   # Alembic configuration for migrations
├── migrations/                   # Database migration system
│   ├── env.py                    # Environment-aware migration config
│   ├── script.py.mako            # Migration template
│   └── versions/                 # Versioned schema changes
│       └── 51677a572fd5_initial_schema_with_sources_and_logs.py
├── requirements.txt              # Enhanced dependencies (gevent, aiodns, etc.)
├── src/                          # Modular application source code
│   ├── config.py                 # Configuration with User-Agent rotation
│   ├── database/                 # Database abstraction layer
│   │   ├── models.py            # SQLAlchemy models (Source, ScrapedData, Log)
│   │   ├── connection.py        # Session management with pooling
│   │   └── __init__.py
│   ├── scraper/                  # High-performance scraping engine
│   │   ├── tasks.py             # Main Celery task with gevent concurrency
│   │   ├── parsers.py           # Modular parser system (generic + extensible)
│   │   ├── compliance.py        # robots.txt compliance checker
│   │   └── __init__.py
│   ├── dashboard/                # Real-time monitoring
│   │   └── app.py               # Streamlit dashboard with analytics
│   └── __init__.py
└── logs/                         # Application logs directory
```

## 5. Core Components Detail

### 5.1 Gevent Worker Configuration
```yaml
# docker-compose.yml worker service
worker:
  command: celery -A src.scraper.tasks worker --loglevel=info --pool=gevent --concurrency=50
  # Key Features:
  # • --pool=gevent: Uses greenlet-based coroutine pool
  # • --concurrency=50: 50 concurrent tasks (adjustable)
  # • Lightweight: Each greenlet ~2-3MB vs thread ~8MB
  # • Non-blocking I/O: Network requests don't block entire worker
```

### 5.2 Modular Parser System (`src/scraper/parsers.py`)
```python
# Generic parser with extensible architecture
def parse_generic(soup: BeautifulSoup) -> dict:
    """Extracts title, headings, links from any HTML"""
    # Can be extended with:
    # • Domain-specific parsers (parse_news, parse_ecommerce)
    # • Schema.org extraction
    # • Microdata/RDFa parsing
    # • Custom extraction rules
```

### 5.3 Enhanced Fetcher with Retry Logic
```python
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(requests.RequestException)
)
def fetch_url(session, url):
    """Intelligent fetching with:
    • 3 retry attempts
    • Exponential backoff (2s, 4s, 8s)
    • User-Agent rotation
    • Connection pooling
    • Timeout handling
    """
```

### 5.4 Database Schema (Optimized for High Volume)
```sql
-- Sources table for domain management
CREATE TABLE sources (
    id SERIAL PRIMARY KEY,
    domain VARCHAR(255) UNIQUE NOT NULL,
    robots_url VARCHAR(255),
    last_crawled TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_domain_active (domain, is_active)
);

-- High-performance scraped data storage
CREATE TABLE scraped_data (
    id SERIAL PRIMARY KEY,
    source_id INTEGER REFERENCES sources(id),
    url VARCHAR UNIQUE NOT NULL,
    title VARCHAR,
    content JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_url (url),
    INDEX idx_source_created (source_id, created_at)
) WITH (autovacuum_enabled = true);

-- Audit logging for debugging
CREATE TABLE logs (
    id SERIAL PRIMARY KEY,
    level VARCHAR(50),
    task_id VARCHAR,
    url VARCHAR,
    message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_task_level (task_id, level),
    INDEX idx_created (created_at DESC)
);
```

## 6. Performance Characteristics

### 6.1 Throughput Estimates
```
Single Worker (50 concurrency):
• Light pages (10KB): ~500 pages/minute
• Medium pages (100KB): ~200 pages/minute
• Heavy pages (1MB): ~50 pages/minute

Scaling Options:
• docker-compose up --scale worker=3 → 150 concurrent tasks
• Add more Redis instances for queue sharding
• Implement database read replicas
```

### 6.2 Resource Requirements
```
Per Worker Container:
• CPU: 0.5-2 cores (depending on parsing complexity)
• Memory: 150-300MB (50 greenlets × 3MB + overhead)
• Network: 10-100Mbps (depending on page sizes)

Database (PostgreSQL):
• Connections: workers × concurrency + buffer
• Storage: JSONB compression for content
• Indexes: Optimized for frequent queries
```

## 7. Deployment & Scaling Commands

### 7.1 Basic Deployment
```bash
# 1. Clone and configure
git clone <repository-url>
cd scrapping-project-for-abtin
cp .env.example .env
# Edit .env with your settings

# 2. Build and start
docker-compose up --build -d

# 3. Run migrations
docker-compose run --rm worker alembic upgrade head

# 4. Access services
# Dashboard: http://localhost:8501
# PostgreSQL: localhost:5432
# Redis: localhost:6379
```

### 7.2 Performance Tuning
```bash
# Scale workers (horizontal scaling)
docker-compose up -d --scale worker=3

# Monitor queue depth
docker-compose exec redis redis-cli LLEN celery

# Check worker status
docker-compose exec worker celery -A src.scraper.tasks status

# View performance metrics
docker stats $(docker ps -q --filter "name=scraper")

# Adjust concurrency (per worker)
# Edit docker-compose.yml: --concurrency=100
```

### 7.3 Database Optimization
```bash
# Monitor database connections
docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB \
  -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';"

# Check table sizes
docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB \
  -c "\dt+"

# Analyze query performance
docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB \
  -c "EXPLAIN ANALYZE SELECT * FROM scraped_data ORDER BY created_at DESC LIMIT 100;"
```

## 8. Advanced Configuration

### 8.1 Environment Variables for Tuning
```bash
# Performance Tuning
CELERYD_CONCURRENCY=50           # Number of concurrent greenlets
CELERYD_POOL=gevent              # Pool type
CELERYD_PREFETCH_MULTIPLIER=1    # Prefetch multiplier for gevent
CELERYD_MAX_TASKS_PER_CHILD=1000 # Restart worker after N tasks

# Database Tuning
POSTGRES_MAX_CONNECTIONS=100     # Max DB connections
POSTGRES_SHARED_BUFFERS=128MB    # Shared buffers
POSTGRES_EFFECTIVE_CACHE_SIZE=1GB # Cache size

# Redis Tuning
REDIS_MAXMEMORY=256mb            # Max memory for Redis
REDIS_MAXMEMORY_POLICY=allkeys-lru # Eviction policy
```

### 8.2 Gevent-Specific Optimizations
```python
# In tasks.py - Gevent monkey patching (optional)
from gevent import monkey
monkey.patch_all()

# Session pooling for HTTP connections
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_session():
    """Create a session with connection pooling and retries"""
    session = requests.Session()
    
    # Connection pooling
    adapter = HTTPAdapter(
        pool_connections=100,
        pool_maxsize=100,
        max_retries=3,
        pool_block=False
    )
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    return session
```

## 9. Monitoring & Alerting

### 9.1 Key Performance Metrics
```python
# Metrics to track:
METRICS = {
    "queue_length": "redis.llen('celery')",  # Pending tasks
    "active_workers": "celery inspect active",  # Running tasks
    "success_rate": "success_count / total_count * 100",
    "avg_response_time": "sum(response_times) / count",
    "error_rate": "error_count / total_count * 100",
    "db_connections": "pg_stat_activity count",
    "memory_usage": "container memory usage"
}
```

### 9.2 Health Checks
```bash
# Worker health
docker-compose exec worker celery -A src.scraper.tasks inspect ping

# Queue health
docker-compose exec redis redis-cli PING

# Database health
docker-compose exec postgres pg_isready -U $POSTGRES_USER -d $POSTGRES_DB

# Dashboard health
curl -f http://localhost:8501/_stcore/health
```

### 9.3 Log Analysis
```bash
# Real-time log monitoring
docker-compose logs -f --tail=100 worker

# Error rate calculation
docker-compose logs worker 2>/dev/null | \
  grep -c "ERROR\|FAILED\|Exception" | \
  awk '{print "Error rate: " $1 " errors in last hour"}'

# Performance bottlenecks
docker-compose logs worker | \
  grep "scrape_task" | \
  awk -F' ' '{print $1, $6}' | \
  sort | uniq -c | sort -rn
```

## 10. Fault Tolerance & Recovery

### 10.1 Automatic Recovery Mechanisms
```python
# In tasks.py - Comprehensive error handling
@app.task(bind=True, autoretry_for=(Exception,), 
          retry_backoff=True, retry_jitter=True, 
          max_retries=3)
def scrape_task(self, url):
    try:
        # Main scraping logic
        pass
    except requests.RequestException as e:
        # Network errors - retry
        self.retry(exc=e, countdown=60)
    except IntegrityError as e:
        # Database conflicts - log and continue
        log_event(db, "WARN", f"Integrity error: {e}", task_id, url)
    except Exception as e:
        # Unknown errors - log and fail
        log_event(db, "ERROR", f"Unhandled: {e}", task_id, url)
        raise
```

### 10.2 Disaster Recovery Procedures
```bash
# 1. Queue recovery (lost tasks)
docker-compose exec redis redis-cli DEL celery

# 2. Database recovery (corrupted data)
docker-compose exec postgres pg_dump -U $POSTGRES_USER $POSTGRES_DB > backup.sql
docker-compose exec postgres psql -U $POSTGRES_USER -d $POSTGRES_DB -c "VACUUM ANALYZE;"

# 3. Worker recovery (stuck processes)
docker-compose restart worker
docker-compose exec worker celery -A src.scraper.tasks purge -f

# 4. Full system reset
docker-compose down -v
docker-compose up -d
docker-compose run --rm worker alembic upgrade head
```

## 11. Extensibility & Future Enhancements

### 11.1 Plugin Architecture
```python
# Future: Plugin-based parser system
PARSERS = {
    'generic': parse_generic,
    'news': parse_news_article,
    'ecommerce': parse_product_page,
    'social': parse_social_media
}

def get_parser(url):
    """Select parser based on URL pattern"""
    if 'news' in url:
        return PARSERS['news']
    elif 'product' in url:
        return PARSERS['ecommerce']
    else:
        return PARSERS['generic']
```

### 11.2 Rate Limiting & Politeness
```python
# Future: Domain-based rate limiting
from redis import Redis
import time

def rate_limit_domain(domain, requests_per_minute=60):
    """Implement domain-specific rate limiting"""
    redis = Redis.from_url(settings.REDIS_URL)
    key = f"rate_limit:{domain}"
    current = redis.incr(key)
    
    if current == 1:
        redis.expire(key, 60)  # Reset every minute
    
    if current > requests_per_minute:
        time.sleep(60 / requests_per_minute)
```

### 11.3 Advanced Caching
```python
# Future: Redis-based caching
def cache_robots_txt(domain, content, ttl=86400):
    """Cache robots.txt for 24 hours"""
    redis = Redis.from_url(settings.REDIS_URL)
    key = f"robots:{domain}"
    redis.setex(key, ttl, content)

def get_cached_robots_txt(domain):
    """Get cached robots.txt"""
    redis = Redis.from_url(settings.REDIS_URL)
    key = f"robots:{domain}"
    return redis.get(key)
```

---

## Summary of Key Advantages

1. **High Performance:** 50x concurrent scraping with gevent greenlets
2. **Fault Tolerant:** Comprehensive retry logic and error handling
3. **Modular Design:** Separated concerns for easy maintenance and extension
4. **Production Ready:** Database migrations, structured logging, monitoring
5. **Scalable:** Horizontal scaling via Docker Compose
6. **Resource Efficient:** Lightweight greenlets vs traditional threads
7. **Real-time Monitoring:** Streamlit dashboard with analytics
8. **Data Integrity:** Atomic operations with proper database transactions
9. **Compliance Aware:** robots.txt checking and user-agent rotation
10. **Developer Friendly:** Hot-reload development, comprehensive documentation

This architecture represents a state-of-the-art web scraping system capable of handling thousands of URLs per hour while maintaining reliability, compliance, and data quality.