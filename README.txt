# Enterprise Web Scraper ğŸ•·ï¸

A high-performance, containerized web scraping platform built with Python, Celery, and Streamlit. Designed for concurrency, resilience, and strict compliance.

![CI Status](https://github.com/YOUR_USERNAME/scrapping-project-for-abtin/actions/workflows/ci.yml/badge.svg)

## ğŸ—ï¸ Architecture

- **Ingestion**: Streamlit Dashboard for bulk URL input with Pydantic validation.
- **Queue**: Redis for task buffering.
- **Workers**: Celery with Gevent pool (50+ threads) for high-speed I/O.
- **Storage**: PostgreSQL with SQLAlchemy ORM and Alembic migrations.
- **Monitoring**: Flower for real-time worker metrics + Custom DB Logging.
- **Resilience**: Robots.txt enforcement, User-Agent rotation, and exponential backoff retries.

## ğŸš€ Getting Started

### Prerequisites
- Docker & Docker Compose

### Running the App
1. Clone the repo:
   ```bash
   git clone https://github.com/YOUR_USERNAME/scrapping-project-for-abtin.git
   cd scrapping-project-for-abtin
   ```

2. Create a `.env` file:
   ```ini
   POSTGRES_USER=admin
   POSTGRES_PASSWORD=adminpass
   POSTGRES_DB=scraper_db
   POSTGRES_HOST=postgres
   REDIS_URL=redis://redis:6379/0
   ```

3. Launch the stack:
   ```bash
   docker compose up --build -d
   ```

4. Access the interfaces:
   - **Dashboard**: http://localhost:8501
   - **Flower (Monitoring)**: http://localhost:5555

## ğŸ§ª Testing

Run the test suite locally:
```bash
pip install -r requirements.txt
pytest tests/ -v
```

## ğŸ› ï¸ Tech Stack

- **Python 3.10**
- **Celery / Redis / PostgreSQL**
- **BeautifulSoup4 / Requests / Tenacity**
- **Streamlit / Pandas**
- **Pytest / GitHub Actions**
```


It has been a pleasure guiding you through this. You have a solid engine nowâ€”happy scraping! ğŸš€
